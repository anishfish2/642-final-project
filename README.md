# 642-final-project
hollow knight RL

In this project, we explore the application of reinforcement learning (RL) algorithms to the game Hollow Knight, specifically focusing on techniques learned in class such as Deep Q-Networks (DQN) and Proximal Policy Optimization (PPO). Hollow Knight is a challenging action-adventure game where players face off against a series of bosses, requiring skillful navigation and combat. We applied RL to model the task of beating these bosses, where the agent’s objective is to defeat the boss while minimizing the number of lives lost. The problem is sequential in nature, as the agent must continuously make decisions based on its current state, with each action affecting subsequent outcomes, there is limited long term planning.
Through our experiments, we found that DQN was able to stably trend towards learning to beat the boss, while the PPO algorithm struggled to learn. We found this particularly interesting considering the dynamic and complex nature of Hollow Knight. Our exploration also involved tuning various parameters and testing different engineered features, as well as experimenting with diverse bosses that lead to drastically different environments. We modeled the agent's environment by providing the screen as pixel input, which the RL model processes to inform decisions. Additionally, we implemented optical flow as a new feature in the observation space to determine whether it improved the agent’s ability to perceive and react to changes in the environment. Our findings suggest that DQN, coupled with optical flow, offers a promising approach for RL applications.

Full Report: https://drive.google.com/drive/folders/1oHrXCuJsk4BufytiTnGT-grZL5PK5WuA?usp=sharing

Project Code: https://drive.google.com/drive/folders/1oHrXCuJsk4BufytiTnGT-grZL5PK5WuA?usp=sharing

Forked from: https://github.com/seermer/HollowKnight_RL

Motivation
The application of reinforcement learning (RL) to video games has garnered significant attention due to its potential to create agents capable of learning complex decision-making strategies in dynamic environments. Hollow Knight, with its intricate gameplay mechanics and challenging boss fights, presents a perfect opportunity to explore the capabilities of RL algorithms. The game requires precise control over character movement, combat decisions, and strategic planning, making it an ideal candidate for training agents using RL.
Our motivation stems from the desire to push the boundaries of RL in action-adventure games like Hollow Knight, where sequential decision-making and long-term strategy are imperative. By implementing algorithms like Deep Q-Networks (DQN) and Proximal Policy Optimization (PPO), we aim to explore how these methods can effectively navigate complex game environments and improve agent performance. Additionally, we seek to enhance the agent’s understanding of the environment by incorporating uncommon features, such as optical flow, to provide richer, motion-based input that could improve decision-making during real-time, fast-paced scenarios.
Through this project, we aim not only to evaluate the effectiveness of DQN and PPO in the context of Hollow Knight but also to contribute to the ongoing research in RL by integrating new techniques and features for evaluation. The goal is to develop an RL agent capable of mastering both the exploration and combat challenges of Hollow Knight, ultimately showcasing the potential of RL to address real-time decision-making in dynamic, complex environments.
Introduction
Reinforcement learning (RL) has emerged as a powerful technique for solving problems in complex, dynamic environments. This project explores the application of RL algorithms to the game Hollow Knight, a challenging action-adventure game known for its intricate boss fights and exploration. In Hollow Knight, players navigate a vast interconnected world, battling tough bosses that require precise movement, timing, and strategy. The sequential nature of the game, where each decision impacts future outcomes, makes it an ideal candidate for RL applications. 
The sequential nature of Hollow Knight is central to the application of RL. Unlike static decision-making problems, where the outcome of each action is immediate and isolated, Hollow Knight involves a series of interconnected decisions that unfold over time. At each step, the agent makes decisions based on its current state, which is typically represented by the screen image or a set of relevant features extracted from the game. The agent must then choose an action—such as moving, attacking, dodging, or jumping—based on the information it has gathered which locks its action for the next few frames. The outcome of this action alters the environment, affecting the agent’s state in the next time step, which may include changes in the position of enemies, the agent's health, or other environmental conditions. This creates a dynamic environment where each action influences future states highly and possible outcomes, requiring the agent to learn from the long-term consequences of its behavior.
The task becomes even more complex due to the exploration vs. exploitation challenge inherent in sequential decision-making. At each decision point, the agent must balance exploring new actions to discover potentially better strategies like hugging the wall to reset its jump to avoid a projectile with exploiting known actions that have yielded positive outcomes like attacking a meteor to deflect it away. In a game like Hollow Knight, where the complexity of enemies and obstacles varies across levels and bosses, finding this balance is crucial for the agent's success.
In this project, we aim to create an  RL that can be used to defeat the various bosses as opposed to most other projects that aim to defeat one boss. We specifically focus on two widely-used RL algorithms: Deep Q-Networks (DQN) and Proximal Policy Optimization (PPO). These algorithms have been successfully applied in a range of environments and domains. We initially hypothesize that PPO, with its emphasis on stable and efficient policy updates, may offer better performance in terms of exploration and convergence speed, particularly in a complex environment like Hollow Knight, where the agent must learn not only to navigate but also to engage in combat.
In addition to comparing DQN and PPO, we also investigate the impact of different parameters and features on the agent’s performance. The agent’s input comes directly from the game’s screen, which provides a high-dimensional observation space. To enhance the agent’s ability to perceive and react to the environment, we integrate optical flow, a computer vision technique that tracks motion between frames. This addition aims to improve the agent's ability to detect movement and changes in the environment, potentially leading to more informed decisions during boss encounters as it should have information on projectile motion.
This introduction sets the stage for our exploration of RL techniques in Hollow Knight, detailing the relevance of the problem, the RL algorithms under consideration, and the innovative features added to enhance the agent’s learning process. The following sections will outline our methodology, experimental design, and the results obtained from training our agent in this challenging environment.
Literature Review
Reinforcement learning (RL) has gained significant attention in recent years, particularly for its application in video games, where it provides a framework for developing intelligent agents that learn to make decisions through interaction with dynamic environments. This section reviews key literature that informs the development and implementation of RL algorithms for training agents in Hollow Knight, with a particular focus on deep reinforcement learning (DRL) techniques such as Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), as well as innovations in state representation and feature extraction.
Shao et al. (2019) provide a comprehensive survey of DRL in video games, discussing various techniques such as value-based, policy gradient, and model-based approaches. Their work highlights the challenges inherent in applying DRL to video games, including high-dimensional state spaces, exploration-exploitation dilemmas, and the need for algorithms that can handle delayed rewards in dynamic environments. The authors emphasize the importance of efficient RL algorithms capable of addressing imperfect / incomplete information, a theme that has similarities with the complexities of Hollow Knight, where agents must navigate intricate environments and combat systems that require real-time decision-making (Shao et al., 2019).
One of the seminal works in the application of DRL to real-time strategy games is the AlphaStar paper by Vinyals et al. (2019), which explores the use of PPO in training an agent to play StarCraft II. This study demonstrates the power of hierarchical architectures combining multiple neural networks for feature extraction and action selection. PPO was instrumental in balancing exploration and exploitation, an essential aspect for learning in high-dimensional and complex environments. These insights are directly applicable to Hollow Knight, as PPO has shown promise in learning optimal policies in similar action-packed environments that demand both strategic planning and real-time adaptability.
The work of Shu et al. (2021) in experience-driven procedural content generation (PCG) for Super Mario Bros (SMB) further enriches the understanding of how RL can be used not just for action selection but also for enhancing game environments. Their framework focuses on generating online game levels tailored to player experience, using latent vectors to represent states and actions. This is particularly relevant for Hollow Knight, where the exploration of diverse game areas is crucial to mastering the game’s challenges. The ability to craft reward functions that promote exploration—rather than just successful actions—may enhance the agent's overall learning process, encouraging creative strategies to overcome difficult bosses and navigate complex terrains (Shu et al., 2021).
In terms of improving the agent's ability to handle state spaces, the DQN literature introduces a frame buffer technique and stores the previous four frames for reference to mimic motion in dynamic environments. This helps the agent stabilize to evaluate movement across frames and make more informed decisions based on changes in the environment. However, an alternative approach, as proposed by Hui et al. (2018), is to incorporate optical flow models, which directly estimate areas of pixel motion. LiteFlowNet, a lightweight optical flow model, provides a promising solution that balances speed and accuracy, the use of the lightweight model allows for real time inference potentially improving the agent’s ability to perceive motion without the computational burden of latency by heavy processing of raw frames. The integration of optical flow into the state space of the agent should lead to more efficient learning and decision-making, especially in environments with fast-moving enemies and complex platforming sequences.
The application of RL to competitive environments is also well-documented in studies involving games like Super Smash Bros. Melee (SSBM) and first-person shooter (FPS) games. These studies focus on the challenges posed by partial observability and the need for intelligent decision-making under complex state dynamics. In SSBM, agents leverage features such as player positions and velocities, rather than relying solely on raw pixel inputs. This feature-centric approach accelerates the learning process by enabling the agent to focus on significant gameplay aspects. Similarly, Lample and Chaplot (2017) in their FPS game study, utilize game-specific features like enemy positions and collectibles to enhance agent performance. Such feature-driven approaches could be beneficial for Hollow Knight as well, where the high-dimensional state (raw pixel data) can be simplified by extracting meaningful features such as enemy positions, health, and item statuses. This would allow the agent to focus on the most relevant elements of the game, improving both training speed and decision-making.
Furthermore, Wang et al. (2016) introduced innovations in DQN, such as the use of dueling network architectures, which separate the estimation of state values and action advantages. This architecture can improve the learning process by allowing the agent to better distinguish between important and less important actions, which is crucial in Hollow Knight, where certain actions (e.g., dodging or jumping) are often more critical than others depending on the boss fight scenario.
The team had planned to explore the use of latent representations for state evaluation, and considered the work on neural discrete representation learning by van den Oord et al. (2017). This research discusses how state spaces, such as the pixel data in Hollow Knight, can be represented in more compact, discrete forms. Techniques like autoencoders could reduce the state space's dimensionality, facilitating faster and more effective learning. By using latent variables to represent states, the agent may be able to learn optimal strategies more quickly and generalize better across different stages of the game. The team had also played around with the thought of substituting the DQN replay buffer with something like a vector database like ChromaDB (https://docs.trychroma.com/). The idea was to store and sample using approximate nearest neighbours which Chroma leverages to generate better samples.
In addition to these techniques, the use of Rainbow DQN, as outlined by Hessel et al. (2017), introduces advanced methods such as prioritized experience replay buffers, where transitions with high expected learning progress are replayed more frequently. This optimization can be incorporated into the Hollow Knight agent to ensure that the most valuable experiences are reinforced, helping the agent to learn faster and more efficiently.
In addition to the general reinforcement learning techniques discussed above, Proximal Policy Optimization (PPO), introduced by Schulman et al. (2017), has been a key algorithm in reinforcement learning due to its stability and efficiency in high-dimensional environments. PPO balances exploration and exploitation by using a surrogate objective function with a clipping mechanism, ensuring that policy updates are stable while still allowing for effective learning. This makes PPO particularly suited for environments like Hollow Knight, where real-time decision-making and adaptability are critical. Schulman et al. demonstrated the utility of PPO in various applications, and its application could provide promising results for training agents in such dynamic, action-packed settings (Schulman et al., 2017).
The team ultimately did not get to incorporate the above two due to circumstances but hope to do so later in their free time. Combining these insights from the literature, this project aims to apply and expand upon existing RL techniques to train an agent capable of mastering the complex environments and boss fights in Hollow Knight. The combination of PPO, DQN, optical flow, and feature extraction methods offers a promising approach for developing a high-performing RL agent that can navigate the challenges of the game’s dynamic environment.

